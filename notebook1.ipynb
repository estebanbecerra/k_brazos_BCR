{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/estebanbecerra/k_brazos_BCR/blob/main/notebook1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "id": "view-in-github"
    },
    {
      "cell_type": "markdown",
      "id": "cde3161a",
      "metadata": {
        "id": "cde3161a"
      },
      "source": [
        "# **Introducción al Problema de los K-Brazos y Métodos de Aprendizaje por Refuerzo**\n",
        "\n",
        "## **1. Contexto del Problema**\n",
        "\n",
        "El problema de los **K-Brazos** (*Multi-Armed Bandit*, MAB) es un problema fundamental en el **Aprendizaje por Refuerzo** y en la toma de decisiones secuenciales bajo incertidumbre. En este escenario, un agente tiene que elegir repetidamente entre **K opciones** (brazos), cada una de las cuales proporciona una recompensa **desconocida y estocástica**.\n",
        "\n",
        "El objetivo del agente es aprender, a partir de la experiencia, **qué brazo seleccionar** para maximizar su recompensa acumulada a lo largo del tiempo. Para ello, debe equilibrar dos estrategias opuestas:\n",
        "\n",
        "- **Exploración:** Probar nuevos brazos para descubrir cuál es el mejor.\n",
        "- **Explotación:** Elegir el brazo que ha dado las mejores recompensas hasta el momento.\n",
        "\n",
        "Este dilema entre **exploración y explotación** es clave en numerosas aplicaciones reales, como la **publicidad online**, la **medicina personalizada**, los **sistemas de recomendación** y la **optimización de estrategias de inversión**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Tipos de Brazos Implementados**\n",
        "\n",
        "Para modelar las recompensas en el problema de los K-Brazos, hemos utilizado tres distribuciones estadísticas diferentes:\n",
        "\n",
        "### **2.1. Brazo con Distribución Normal**\n",
        "Cada brazo genera recompensas que siguen una **distribución normal** con media **μ** y desviación estándar **σ**:\n",
        "\n",
        "**X ∼ N(μ, σ²)**\n",
        "\n",
        "- Se usa cuando las recompensas pueden tomar valores continuos.\n",
        "- Ejemplo: **Tiempo de respuesta de un servidor, precios en bolsa, etc.**\n",
        "\n",
        "### **2.2. Brazo con Distribución Bernoulli**\n",
        "Cada brazo devuelve una recompensa **binaria** (0 o 1) con probabilidad **p**:\n",
        "\n",
        "**X ∼ Bernoulli(p)**\n",
        "\n",
        "- Se emplea en problemas donde cada acción tiene **éxito o fracaso**.\n",
        "- Ejemplo: **Un usuario hace clic en un anuncio o no.**\n",
        "\n",
        "### **2.3. Brazo con Distribución Binomial**\n",
        "Cada brazo devuelve una recompensa que sigue una **distribución Binomial** con **n** intentos y probabilidad de éxito **p**:\n",
        "\n",
        "**X ∼ Bin(n, p)**\n",
        "\n",
        "- Se usa en contextos donde una acción puede tener múltiples intentos exitosos dentro de un periodo.\n",
        "- Ejemplo: **Cantidad de clics en un anuncio después de n visualizaciones.**\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Algoritmos Utilizados**\n",
        "\n",
        "Para resolver el problema de los K-Brazos, se han implementado diferentes estrategias de selección de brazos:\n",
        "\n",
        "### **3.1. Algoritmo Epsilon-Greedy**\n",
        "El algoritmo **ε-Greedy** combina exploración y explotación de forma simple:\n",
        "- Con probabilidad **ε**, selecciona un brazo **aleatoriamente** (exploración).\n",
        "- Con probabilidad **1 - ε**, selecciona el brazo con **mayor recompensa esperada** (explotación).\n",
        "\n",
        "Parámetro clave:\n",
        "- **ε (epsilon)**: Controla el grado de exploración (valores típicos: **0.1, 0.01, 0**).\n",
        "\n",
        "**Ventajas:**\n",
        "Simple de implementar.  \n",
        "Funciona bien en la práctica con un ajuste adecuado de **ε**.  \n",
        "\n",
        "**Desventajas:**\n",
        "No ajusta dinámicamente la exploración.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3.2. Algoritmo UCB (Upper Confidence Bound)**\n",
        "El algoritmo **UCB (Upper Confidence Bound)** **balancea exploración y explotación** de manera más eficiente utilizando una estrategia basada en la **incertidumbre** de cada brazo.\n",
        "\n",
        "- Se calcula un **intervalo de confianza superior** para cada brazo.\n",
        "- Se selecciona el brazo con el **mayor límite superior**, favoreciendo la exploración al inicio y la explotación posteriormente.\n",
        "\n",
        "Dos variantes implementadas:\n",
        "1. **UCB1:** Calcula los límites superiores como:\n",
        "\n",
        "   **UCB_i = μ̂_i + c √( log(t) / N_i )**\n",
        "\n",
        "   donde **c** controla la exploración.\n",
        "\n",
        "2. **UCB2:** Introduce un ajuste dinámico basado en la duración de las fases de exploración.\n",
        "\n",
        "**Ventajas:**\n",
        "Reduce la exploración innecesaria con el tiempo.  \n",
        "Tiene mejores garantías teóricas que **ε-Greedy**.  \n",
        "\n",
        "**Desventajas:**\n",
        "Requiere más cálculos en comparación con **ε-Greedy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3.3. Algoritmos Basados en Gradiente**\n",
        "Los algoritmos basados en **ascenso de gradiente** actualizan las preferencias de cada brazo basándose en la recompensa obtenida.\n",
        "\n",
        "1. **Softmax:** Asigna probabilidades a cada brazo según la regla de **softmax**:\n",
        "\n",
        "   **P(a_i) = exp(H_i) / Σ exp(H_j)**\n",
        "\n",
        "   donde **H_i** es la preferencia del brazo **i**, actualizada tras cada iteración.\n",
        "\n",
        "2. **Gradient Bandit:** Variante que ajusta las preferencias utilizando una tasa de aprendizaje.\n",
        "\n",
        "**Ventajas:**\n",
        "Funciona bien en problemas con recompensas más estructuradas.  \n",
        "Ajusta dinámicamente la exploración y explotación.  \n",
        "\n",
        "**Desventajas:**\n",
        "Sensible a la elección de la tasa de aprendizaje.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4. Métricas de Evaluación**\n",
        "Para comparar el rendimiento de los algoritmos, se utilizan las siguientes métricas:\n",
        "\n",
        "1. **Recompensa Promedio**   \n",
        "   - Muestra **cómo evoluciona la recompensa media** obtenida por cada algoritmo con el tiempo.\n",
        "   - Indica qué estrategia aprende **más rápido** y obtiene **mejores recompensas**.\n",
        "\n",
        "2. **Porcentaje de Selección del Brazo Óptimo**  \n",
        "   - Indica **con qué frecuencia cada algoritmo selecciona el mejor brazo**.\n",
        "   - Un valor alto implica que el algoritmo **aprendió bien** cuál es la mejor opción.\n",
        "\n",
        "3. **Regret Acumulado**   \n",
        "   - Mide **la pérdida acumulada** por no elegir siempre el brazo óptimo.\n",
        "   - Cuanto **menor sea el regret**, mejor es el algoritmo.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Conclusión**\n",
        "Este estudio analiza distintos enfoques para resolver el problema de los **K-Brazos**, comparando estrategias clásicas como **ε-Greedy**, métodos basados en **UCB** y enfoques de **ascenso de gradiente**. Además, se han evaluado distintas distribuciones de recompensa para entender su impacto en el aprendizaje del agente.\n",
        "\n",
        "**Resumen de resultados esperados:**\n",
        "- **ε-Greedy (ε=0.1)** suele ser una opción robusta en muchos escenarios.\n",
        "- **UCB** ofrece una mejor exploración adaptativa con el tiempo.\n",
        "- **Métodos de Gradiente** pueden ser útiles cuando las recompensas siguen distribuciones más complejas.\n",
        "- **La elección de la distribución de los brazos afecta significativamente la velocidad de aprendizaje y el rendimiento de los algoritmos**.\n",
        "\n",
        "En los siguientes notebooks, realizaremos **estudios detallados** para cada combinación de algoritmos y distribuciones, evaluando su impacto y rendimiento.\n",
        "\n",
        "---\n",
        "\n",
        "**Nota:** Todos los experimentos han sido implementados en **Python** utilizando librerías como `numpy` y `matplotlib`, organizados en módulos para facilitar su análisis y ejecución.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}